---
output: pdf_document
---

Last time we got familiar with basic concept of UNIX shell bash. Now we would like to learn how to compute on cluster and how to behave there. Here are more materials about the cluster we will use [Vital-it](http://edu.isb-sib.ch/course/view.php?id=72).

# Connection to server

### OS X, Linux users

**connection**

Open terminal and connect to Vital-it using `ssh` (secure shell)

```bash
ssh <username>@prd.vital-it.ch
```

**file transfer**

To copy file **to** cluster you have to be in the terminal of your computer using `scp` (secure copy)

```bash
scp <path>/<filename> <username>@prd.vital-it.ch:/<path>/<filename>
```

to copy file **from** the cluster to your computer, you still have to be in the terminal of your computer, but the order of locations in the command will be just reverse

```bash
scp <username>@prd.vital-it.ch:/<path>/<filename> <path>/<filename>
```

### Windows users

**connection**

1. Install [WinSCP](http://winscp.net/eng/download.php) and [Putty](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)
2. First start WinSCP by:
    -	hostname: prd.vital-it.ch
    -	username:
    -	password:
3. Then start your Putty entering hostname prd.vital-it.ch

**file transfer**

Transfer the file to your computer by drag and drop from the server side. Every time in the exercise you see `scp` command, it is only for Mac/Linux users, you use WinSCP. In the WinSCP navigate your paths and check your folders.

***

\newpage

# Quality control

## Prepare a workspace

Keep in mind that you can save a lot of energy and mistakes by using `<tab>` for auto completion of all existing paths you type and by using your command history (up and down arrows and `Ctrl+r`)

1. create your scratch directory (`/scratch/beegfs/monthly/mls_2017/<username>`) and go there (`cd <path>`)

The list of samples and corresponding group numbers (if your group is not listed :

```{R, echo = F}
samples <- c('ESL0284','ESL0366','ESL0367','ESL0368','ESL0378','ESL0379','ESL0380','ESL0382','ESL0385','ESL0387','ESL0390','ESL0392')
data.frame(sample = samples, group = c(paste(1:5, ' + ', 12:17), as.character(6:11)))
```
<!-- Can you figure out why do you have two files? What could possibly R1 and R2 mean?? -->
Each group should have two files, R1 and R2.

2. Copy your reads (`/scratch/beegfs/monthly/mls_2017/data/raw_reads/<sample>_R*`) to your scratch.

<sub>
The symbol `*` means "all files". `<sample>_R*` means all the files starting by `<sample>_R*`.
</sub>

***

## Explore genomic reads

The files you got is a compressed text files, if you try `head <file_name>`, you will get just meaningless characters. There are two solutions, either you can uncompress your files by command `gunzip` or preferably you can use command `zcat` to print the file on the standard output. Do not forget that it is still a half GB file each - a lot of letters, you better pipe them somewhere.

1. Format `.fastq` is just formatted plain text formats, so you could use all line tools for parsing text. Check the format with `head`

```bash
zcat <read_file.fastq.gz> | head
```

Every read is represented by four lines: header line, sequence line, quality of basecalling (in PHRED score) and comments (usually blank line).

2. How many reads are in the files `R1` and `R2`? Is it the same number?

***

### Computing on cluster

To compute on cluster you will need two main commands: `bsub` to submit a job and `bjobs` to obtain job status. Submission of a job have the following syntax

```bash
bsub <bsub_parameters> <command> <command_parameters>
```

Every time, you run a `bsub` on the front-end, it will put your job in the queue and assign you a computer once, there will be one free for you, given specifications you have provided in `<bsub_parameters>`.

1. Submit a job, that will compute number of reads are in the file `R1`.

```
bsub '<command_to_calculate_num_of_reads_in_R1> > number_of_reads_in_R1'
```

2. check `bjobs`.
3. check if file `number_of_adapters` was created and if the result in the same number as the .

***

### Modules

The second advantage of Vital-it cluster is, that there are installed modules with various programs. A software available on Vital-it you can loaded using

```bash
module add <field>/<category>/<program>/<version>
```

The list of programs and commands for loading them can be found on [Vital-it webpage](http://www.vital-it.ch/services/software).

1. Find there a command for program `fastqc` and execute it.

<sub>Vital-it trick: An alternative option for webpage interface is a command `vit_soft`, `vit_soft -s <name>` will return a list of packages containing the <name> you specified. To get a command for loading package you have to choose a package from the list and execute `vit_soft -p <package_name>`<sub\>

2. Now, check how your `PATH` has changed.

Adding module is nothing more than just adding a new path to list of directories searched for binaries.

***

## Submit a quality control job

Reads from the sequencing center are just roughly parsed, it is on users to make sure, that reads are without any problem. There are pretty efficient tools checking various aspects of read quality, one of them is `fastqc`, which is already installed on Vital-it. `fastqc` computes a quality control and outputs it to existing output folder defined by parameter `-o`.

```bash
fastqc -o <fastqc_results> <sequencing_reads>
```

1. Activate Vital-IT module `UHTS/Quality_control/fastqc/0.11.2` using command `module add`
2. Create a folder `<fastqc_results>` for an output of `fastqc` and submit a job computing a quality control of read files of yours

```
bsub <parameters> 'command of the job'
```

3. Check the status of the submitted job

BREAK!!!

***

\newpage

## Read a quality control output

0. Copy the quality reports from fastqc to your computer using `scp` or `WinSCP` and open the `.html` file on your computer.
1. How many reads you have?
2. What is the length of reads?
3. Is average quality per base the same along the length of read?
4. Are there any sequencing adapters present in the dataset?
5. Could you approximate a coverage we have given the length of the genome is 1.5 - 2.5 Mbp. For the calculation be conservative, compute the minimal coverage you have.

***

# Adapter trimming and quality filtering

Later on, we are going to "glue" reads together into nice assembly, this process is rather complicated, therefore we need to help assembler as much as possible by giving it as good data as possible. Besides trimming of addapters, we can reduce number of errors by filtering and trimming reads according to quality. A one handy tool integrating all those read pre processing steps is `trimmomatic`.

1. Activate Vital-it module `trimmomatic`

```bash
module add UHTS/Analysis/trimmomatic/0.33
```

2. Copy file with adapters that might be inside the sequencing

```bash
cp /scratch/beegfs/monthly/mls_2017/adapeters/TruSeq3-PE.fa .
```

3. Submit a trimming job (this will take some minutes)

```bash
bsub -n 8 -M 4194304 -J <job_name> '
trimmomatic PE -threads 8 \
<sample>_R1.fastq.gz <sample>_R2.fastq.gz \
<sample>_R1t.fq.gz <sample>_R1np.fq.gz \
<sample>_R2t.fq.gz <sample>_R2np.fq.gz \
ILLUMINACLIP:TruSeq3-PE.fa:3:25:6 LEADING:9 TRAILING:9 SLIDINGWINDOW:4:15 MINLEN:60 \
&> trimming_<sample>.log'
```

<sub>
Those adapters are coming from situations when a DNA fragment is really small, there can be more consecutive adapters sequenced. Those adapters are not cut in demultiplexing in the sequencing centre. It is up on us, to get rid of them. The most of possible adapters of Illumina sequencing are in the file `<adapters.fa>`. `trimmomatic` can try to identify those sequences in the reads and cut them off if the module `ILLUMINACLIP` is used. You have to specify the file with adapters. Other modules of trimmomatic, `LEADING`, `TRAILING` are cutting given number bases from the start and end of read if the quality is below some internal threshold. Module `SLIDINGWINDOW` performs a sliding window trimming, cutting once the average quality within the window falls below a threshold. Module `MINLEN` simply discards all reads which are after all that trimming shorter than given number of bases.
<sub\>

***

## Quality control of trimmed reads

Trimmomatic produces 4 files, (`<sample>_R1t.fq.gz`, `<sample>_R1np.fq.gz`, `<sample>_R2t.fq.gz`, `<sample>_R2np.fq.gz`), these four files are corresponding to trimmed R1 reads, R1 orphan (a read pairs, where `R2` read was filtered out), trimmed R2 reads, and R2 orphans. Use `zcat` and `wc -l` to find out how many reads are in these four files (`R1t` and `R2t` should have exactly same).

Right now you have to verify, that trimmer have successfully trimmed your reads.

1. activate again module for `fastqc`
2. submit a job computing a quality control of trimmed reads
3. download the `.html` reports and check if reads look better now. What is the distribution of length? How many reads were discarded? Is quality dropout in the tail of reads better now?

***

## Hands-on adapter search

Searching for adapter contamination is not that hard since, the sequences are actually known. If you dig into source code of fasqc, or check Illumina website, you can find seqeunces of adapters and usually you find an adapter matching your adapter contamination. Another aproach would be to check kmer frequencies.

1. To identify whole adapter sequence, you can select fulltext search in some subset of your reads (100000 lines). In the subset you use a fulltext search (grep) to find reads containing the adapter sequence (overrepresented kmer), select first one hundred of them and show them using `less`.

```bash
zcat <reads1.fastq.gz> | head -n 100000 | grep <overrepresented kmer> | head -n 100 | less
```

Command `less` allow you a fulltext search as well. Press `/` and copy-paste there the sequence `<overrepresented kmer>`, hit `Enter`. Now all `<overrepresented kmer>` sequences will be highlighted. Can you find a pattern? Do we know whole seqeuence of adapter or is it actually longer?

<sub>
We will actually cut all possible adapters without specifying which one is in our sequence. Therefore we wont use the output of our little adapter investigaton.
<sub\>


Credits: made by Kamil based on materials of the last semester. Contact: kamil.jaron@gmail.com
